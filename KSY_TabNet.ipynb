{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용한 TabNet 라이브러리는 [PyTorch-TabNet](https://github.com/dreamquark-ai/tabnet) 으로 scikit-learn 인터페이스를 따르고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_row', 50)\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306795, 10)\n",
      "(76699, 10)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/'\n",
    "\n",
    "train = pd.read_csv(path+'ksy_train_rating_10n.csv')\n",
    "test = pd.read_csv(path+'ksy_test_rating_10n.csv')\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['user_id'] = train['user_id'].astype('str')\n",
    "train['years'] = train['years'].astype('str')\n",
    "train['fix_age'] = train['fix_age'].astype('str')\n",
    "\n",
    "test['user_id'] = test['user_id'].astype('str')\n",
    "test['years'] = test['years'].astype('str')\n",
    "test['fix_age'] = test['fix_age'].astype('str')\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns:\n",
    "    if train.dtypes[col] == 'object':\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "_data = pd.concat([train, test])\n",
    "categorical_columns = categorical_columns#[2:]\n",
    "for col in categorical_columns:\n",
    "    tem = {id:idx for idx, id in enumerate(_data[col].unique())}\n",
    "    categorical_dims[col] = len(tem)\n",
    "    train[col] = train[col].map(tem)\n",
    "    test[col] = test[col].map(tem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define categorical features for categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                59803\n",
       "isbn                  129777\n",
       "rating                    10\n",
       "book_author             2102\n",
       "publisher               1217\n",
       "language                  11\n",
       "category_high            209\n",
       "years                      5\n",
       "fix_location_state       234\n",
       "fix_age                    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'rating'\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(categorical_columns)]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(categorical_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your embedding sizes : here just a random choice (default =1)\n",
    "# cat_idxs나 cat_dims와 동일한 형태로 만들어야함 or int\n",
    "# cat_emb_dim = [10, 10, 10, 10, 10, 10, 10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "\n",
    "clf = TabNetRegressor(  cat_idxs=cat_idxs,\n",
    "                        cat_dims=cat_dims,\n",
    "                        cat_emb_dim=10,\n",
    "                        optimizer_fn=torch.optim.Adam,  # (default=torch.optim.Adam)\n",
    "                        optimizer_params=dict(lr=2e-2), # (default=dict(lr=2e-2))\n",
    "                        scheduler_params={\"step_size\":10,\n",
    "                                            \"gamma\":0.95},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,   # (default=None)\n",
    "                        mask_type='entmax', # \"sparsemax\", entmax\n",
    "                        n_steps=5,  # (default=3) 이게 부스팅 단계 조절 파라미터! 보통 3~10 정도 사용한대\n",
    "                        device_name = 'cuda',\n",
    "                        seed = 42\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train[categorical_columns], train[target], test_size=0.2)\n",
    "\n",
    "n_targets = 1\n",
    "\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "# X와 y의 형태 맞춰주기 -> n_tragets 값은 변경 가능\n",
    "y_train = np.transpose(np.tile(y_train, (n_targets,1)))\n",
    "\n",
    "X_valid = X_valid.values\n",
    "y_valid = y_valid.values\n",
    "y_valid = np.transpose(np.tile(y_valid, (n_targets,1)))\n",
    "\n",
    "X_test = test[categorical_columns].values\n",
    "y_test = test[target].values\n",
    "y_test = np.transpose(np.tile(y_test, (n_targets,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1000 if not os.getenv(\"CI\", False) else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 7.2959  | train_rmse: 2.41906 | valid_rmse: 2.41509 |  0:00:42s\n",
      "epoch 1  | loss: 5.83729 | train_rmse: 2.3667  | valid_rmse: 2.38357 |  0:01:17s\n",
      "epoch 2  | loss: 5.41863 | train_rmse: 2.20293 | valid_rmse: 2.32913 |  0:01:58s\n",
      "epoch 3  | loss: 4.6854  | train_rmse: 1.98883 | valid_rmse: 2.31756 |  0:02:39s\n",
      "epoch 4  | loss: 3.92533 | train_rmse: 1.78254 | valid_rmse: 2.29335 |  0:03:19s\n",
      "epoch 5  | loss: 3.2928  | train_rmse: 1.61805 | valid_rmse: 2.34874 |  0:03:59s\n",
      "epoch 6  | loss: 2.8126  | train_rmse: 1.49865 | valid_rmse: 2.41215 |  0:04:41s\n",
      "epoch 7  | loss: 2.42915 | train_rmse: 1.4091  | valid_rmse: 2.39286 |  0:05:22s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m----> 3\u001b[0m clf\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      4\u001b[0m     X_train\u001b[39m=\u001b[39;49mX_train, y_train\u001b[39m=\u001b[39;49my_train,\n\u001b[1;32m      5\u001b[0m     eval_set\u001b[39m=\u001b[39;49m[(X_train, y_train), (X_valid, y_valid)],\n\u001b[1;32m      6\u001b[0m     eval_name\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvalid\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      7\u001b[0m     eval_metric\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mrmse\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m#['rmsle', 'mae', 'rmse', 'mse']\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mmax_epochs,\n\u001b[1;32m      9\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m# 최소한 10번은 하자.\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, virtual_batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m     drop_last\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# (default=False)\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:241\u001b[0m, in \u001b[0;36mTabModel.fit\u001b[0;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epochs):\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m     \u001b[39m# Call method on_epoch_begin for all callbacks\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_epoch_begin(epoch_idx)\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(train_dataloader)\n\u001b[1;32m    243\u001b[0m     \u001b[39m# Apply predict epoch to all eval sets\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, valid_dataloader \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eval_names, valid_dataloaders):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:457\u001b[0m, in \u001b[0;36mTabModel._train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m    455\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_begin(batch_idx)\n\u001b[0;32m--> 457\u001b[0m     batch_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_batch(X, y)\n\u001b[1;32m    459\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_container\u001b[39m.\u001b[39mon_batch_end(batch_idx, batch_logs)\n\u001b[1;32m    461\u001b[0m epoch_logs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mparam_groups[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/abstract_model.py:495\u001b[0m, in \u001b[0;36mTabModel._train_batch\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mparameters():\n\u001b[1;32m    493\u001b[0m     param\u001b[39m.\u001b[39mgrad \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(X)\n\u001b[1;32m    497\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(output, y)\n\u001b[1;32m    498\u001b[0m \u001b[39m# Add the overall sparsity loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:586\u001b[0m, in \u001b[0;36mTabNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedder(x)\n\u001b[0;32m--> 586\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtabnet(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:471\u001b[0m, in \u001b[0;36mTabNetNoEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    470\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 471\u001b[0m     steps_output, M_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m    472\u001b[0m     res \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(torch\u001b[39m.\u001b[39mstack(steps_output, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    474\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_multi_task:\n\u001b[1;32m    475\u001b[0m         \u001b[39m# Result will be in list format\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_tabnet/tab_network.py:153\u001b[0m, in \u001b[0;36mTabNetEncoder.forward\u001b[0;34m(self, x, prior)\u001b[0m\n\u001b[1;32m    150\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_bn(x)\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m prior \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     prior \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mones(x\u001b[39m.\u001b[39;49mshape)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    155\u001b[0m M_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    156\u001b[0m att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_splitter(x)[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_d :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['rmse'], #['rmsle', 'mae', 'rmse', 'mse']\n",
    "    max_epochs=max_epochs,\n",
    "    patience=10, # 최소한 10번은 하자.\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=True, # (default=False)\n",
    ")\n",
    "\n",
    "# default loss_fn: mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BEST VALID SCORE FOR bookrec - best epoch {clf.best_epoch}: {clf.best_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['rating'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "now = time.localtime()\n",
    "now_date = time.strftime('%Y%m%d', now)\n",
    "now_hour = time.strftime('%X', now)\n",
    "save_time = now_date + '_' + now_hour.replace(':', '')\n",
    "\n",
    "submit.to_csv('submit/KCH_{}_TabNet_{}{}_{}epoch_{:.4f}.csv'.format(save_time, clf.mask_type, clf.n_steps, clf.best_epoch, clf.best_cost, index = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global explainability : feat importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = clf.explain(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, clf.n_steps, figsize=(20,20))\n",
    "\n",
    "for i in range(clf.n_steps):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['rating'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 코드를 그냥 가져온 것으로 조금 수정이 필요할 듯\n",
    "\"\"\"\n",
    "# save tabnet model\n",
    "saving_path_name = \"./tabnet_model_test_1\"\n",
    "saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "# define new model with basic parameters and load state dict weights\n",
    "loaded_clf = TabNetClassifier()\n",
    "loaded_clf.load_model(saved_filepath)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
