{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_row', 50)\n",
    "\n",
    "path = '../data/'\n",
    "\n",
    "train = pd.read_csv(path + 'train_ratings.csv')\n",
    "test = pd.read_csv(path + 'test_ratings.csv')\n",
    "books = pd.read_csv(path + 'books.csv')\n",
    "users = pd.read_csv(path + 'users.csv')\n",
    "\n",
    "books['img_path'] = books['img_path'].apply(lambda x: path+x)\n",
    "\n",
    "def image_vector(path):\n",
    "    img = Image.open(path)\n",
    "    scale = transforms.Resize((32, 32))\n",
    "    tensor = transforms.ToTensor()\n",
    "    img_fe = Variable(tensor(scale(img)))\n",
    "    return img_fe\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "lr = 0.005\n",
    "EPOCH = 3\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149570/149570 [01:12<00:00, 2050.50it/s]\n"
     ]
    }
   ],
   "source": [
    "idxs = []\n",
    "datas = []\n",
    "\n",
    "for idx in tqdm(range(len(books))):\n",
    "    idxs.append(idx)\n",
    "    data = image_vector(books.iloc[idx, -1])\n",
    "    if data.size()[0] == 3:\n",
    "        data = np.array(data)\n",
    "    else:\n",
    "        data = np.array(data.expand(3, data.size()[1], data.size()[2]))\n",
    "    datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBaseDataset(Dataset):\n",
    "    def __init__(self, idxs, datas):\n",
    "        self.idx = idxs\n",
    "        self.img = datas\n",
    "\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.idx)\n",
    "        \n",
    "    def __getitem__(self, i): \n",
    "        return {'isbn' : torch.tensor(self.idx[i], dtype=torch.float32),\n",
    "                'img' : torch.tensor(self.img[i], dtype=torch.float32),}\n",
    "\n",
    "dataset = MyBaseDataset(idxs, datas)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "pred_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 14, 5, padding = 2),\n",
    "            nn.BatchNorm2d(14),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14, 7, 5, padding = 2),\n",
    "            nn.BatchNorm2d(7),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            nn.Linear(7 * 32 * 32, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder_1 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 7 * 32 * 32),\n",
    "            nn.BatchNorm1d(7 * 32 * 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            nn.Conv2d(7, 14, 5, padding = 2),\n",
    "            nn.BatchNorm2d(14),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14, 3, 5, padding = 2),\n",
    "        )\n",
    "\n",
    "        \n",
    "    \n",
    "  #인코더와 디코더 연산을 차례대로 수행하도록 설정 \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder_1(x)\n",
    "        encoded = encoded.view(-1, 7 * 32 * 32)\n",
    "        encoded = self.encoder_2(encoded)\n",
    "\n",
    "        decoded = self.decoder_1(encoded)\n",
    "        decoded = decoded.view(-1, 7, 32, 32) \n",
    "        decoded = self.decoder_2(decoded)\n",
    "        return decoded, encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8158587705441843e-05\n",
      "3.071391022636936e-05\n",
      "2.8328177395045157e-05\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder(5).to(device)\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        data = data['img'].to(device)\n",
    "       \n",
    "        optimizer.zero_grad() # 매개변수를 0으로 만듭니다.\n",
    "\n",
    "        outputs, _ = model(data) # 입력값을 넣어 순전파를 진행시킨뒤 결과값 배출\n",
    "        _loss = loss(outputs, data) # 결과와 실제 값을 손실함수에 대입\n",
    "        _loss.backward() # 손실함수에서 역전파 수행\n",
    "        optimizer.step() # 옵티마이저를 사용해 매개변수 최적화\n",
    "\n",
    "        running_loss += _loss.item()\n",
    "    \n",
    "    print(running_loss/len(books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_url</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_path</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>../data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>0.111604</td>\n",
       "      <td>2.208946</td>\n",
       "      <td>10.051038</td>\n",
       "      <td>33.557636</td>\n",
       "      <td>43.056538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['1940-1949']</td>\n",
       "      <td>Here, for the first time in paperback, is an o...</td>\n",
       "      <td>../data/images/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>-60.860321</td>\n",
       "      <td>-66.590637</td>\n",
       "      <td>40.232693</td>\n",
       "      <td>-30.225893</td>\n",
       "      <td>-28.444817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Medical']</td>\n",
       "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
       "      <td>../data/images/0374157065.01.THUMBZZZ.jpg</td>\n",
       "      <td>-54.511848</td>\n",
       "      <td>0.230044</td>\n",
       "      <td>56.543148</td>\n",
       "      <td>143.358246</td>\n",
       "      <td>168.976364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>../data/images/0399135782.01.THUMBZZZ.jpg</td>\n",
       "      <td>-60.860321</td>\n",
       "      <td>-66.590637</td>\n",
       "      <td>40.232693</td>\n",
       "      <td>-30.225893</td>\n",
       "      <td>-28.444817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0425176428</td>\n",
       "      <td>What If?: The World's Foremost Military Histor...</td>\n",
       "      <td>Robert Cowley</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425176428.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['History']</td>\n",
       "      <td>Essays by respected military historians, inclu...</td>\n",
       "      <td>../data/images/0425176428.01.THUMBZZZ.jpg</td>\n",
       "      <td>-27.195101</td>\n",
       "      <td>-23.115290</td>\n",
       "      <td>8.017411</td>\n",
       "      <td>-34.547516</td>\n",
       "      <td>-24.669533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         isbn                                         book_title  \\\n",
       "0  0002005018                                       Clara Callan   \n",
       "1  0060973129                               Decision in Normandy   \n",
       "2  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "3  0399135782                             The Kitchen God's Wife   \n",
       "4  0425176428  What If?: The World's Foremost Military Histor...   \n",
       "\n",
       "            book_author  year_of_publication                 publisher  \\\n",
       "0  Richard Bruce Wright               2001.0     HarperFlamingo Canada   \n",
       "1          Carlo D'Este               1991.0           HarperPerennial   \n",
       "2      Gina Bari Kolata               1999.0      Farrar Straus Giroux   \n",
       "3               Amy Tan               1991.0          Putnam Pub Group   \n",
       "4         Robert Cowley               2000.0  Berkley Publishing Group   \n",
       "\n",
       "                                             img_url language       category  \\\n",
       "0  http://images.amazon.com/images/P/0002005018.0...       en  ['Actresses']   \n",
       "1  http://images.amazon.com/images/P/0060973129.0...       en  ['1940-1949']   \n",
       "2  http://images.amazon.com/images/P/0374157065.0...       en    ['Medical']   \n",
       "3  http://images.amazon.com/images/P/0399135782.0...       en    ['Fiction']   \n",
       "4  http://images.amazon.com/images/P/0425176428.0...       en    ['History']   \n",
       "\n",
       "                                             summary  \\\n",
       "0  In a small town in Canada, Clara Callan reluct...   \n",
       "1  Here, for the first time in paperback, is an o...   \n",
       "2  Describes the great flu epidemic of 1918, an o...   \n",
       "3  A Chinese immigrant who is convinced she is dy...   \n",
       "4  Essays by respected military historians, inclu...   \n",
       "\n",
       "                                    img_path         v1         v2         v3  \\\n",
       "0  ../data/images/0002005018.01.THUMBZZZ.jpg   0.111604   2.208946  10.051038   \n",
       "1  ../data/images/0060973129.01.THUMBZZZ.jpg -60.860321 -66.590637  40.232693   \n",
       "2  ../data/images/0374157065.01.THUMBZZZ.jpg -54.511848   0.230044  56.543148   \n",
       "3  ../data/images/0399135782.01.THUMBZZZ.jpg -60.860321 -66.590637  40.232693   \n",
       "4  ../data/images/0425176428.01.THUMBZZZ.jpg -27.195101 -23.115290   8.017411   \n",
       "\n",
       "           v4          v5  \n",
       "0   33.557636   43.056538  \n",
       "1  -30.225893  -28.444817  \n",
       "2  143.358246  168.976364  \n",
       "3  -30.225893  -28.444817  \n",
       "4  -34.547516  -24.669533  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_vector = pd.DataFrame()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(pred_loader):\n",
    "        _, out_data = model(data['img'].to(device))\n",
    "        image_vector = pd.concat([image_vector, pd.DataFrame(out_data.squeeze().detach().cpu().numpy())])\n",
    "\n",
    "image_vector.columns = ['v1','v2','v3','v4','v5']\n",
    "image_vector.reset_index(drop = True, inplace= True)\n",
    "books = pd.concat([books, image_vector], axis = 1)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_csv(path+'ksy_books_img.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
